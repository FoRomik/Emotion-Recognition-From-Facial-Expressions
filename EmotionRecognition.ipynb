{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import logging\n",
    "from math import ceil\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/avinashkaur/Desktop/emorec/fer2013/fer2013.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Know your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35887, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out what else is there in the 'Usage' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['PrivateTest', 'PublicTest', 'Training'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(data[\"Usage\"].values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of images in the training dataset therefore is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28709\n"
     ]
    }
   ],
   "source": [
    "print(len(data[(data.Usage)==\"Training\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = data[data.Usage == \"Training\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels_values = train_data.pixels.str.split(\" \").tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making it a dataframe for ease of computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pixels_values = pd.DataFrame(pixels_values, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = pixels_values.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = images.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28709, 2304)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define a function to show image through 48*48 pixels\n",
    "import matplotlib.image as mpimg\n",
    "def show(img):\n",
    "    show_image = img.reshape(48,48)\n",
    "    imgplot = plt.imshow(show_image, cmap = 'gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x135dbb490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#show one image\n",
    "show(images[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per image normalization is carried out (Across the columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = images - images.mean(axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling so that they remain in similar ranges. This ensures that our gradients don't go out of control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = np.multiply(images,100.0/255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take mean of every pixel i.e. mean across the rows of the images array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "each_pixel_mean = images.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "each_pixel_std = np.std(images, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the overall image array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = np.divide(np.subtract(images,each_pixel_mean), each_pixel_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28709, 2304)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flat pixel values is 2304\n"
     ]
    }
   ],
   "source": [
    "image_pixels = images.shape[1]\n",
    "print('Flat pixel values is %d'%(image_pixels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_width = image_height = np.ceil(np.sqrt(image_pixels)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_flat = train_data[\"emotion\"].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_count = np.unique(labels_flat).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of different facial expressions is 7\n"
     ]
    }
   ],
   "source": [
    "print('The number of different facial expressions is %d'%labels_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = dense_to_one_hot(labels_flat, labels_count)\n",
    "labels = labels.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images:  28709\n",
      "Images in the validation set: 1709\n"
     ]
    }
   ],
   "source": [
    "# split data into training & validation\n",
    "print(\"Total images: \", len(images))\n",
    "VALIDATION_SIZE = 1709\n",
    "print(\"Images in the validation set:\", VALIDATION_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_images = images[:VALIDATION_SIZE]\n",
    "validation_labels = labels[:VALIDATION_SIZE]\n",
    "\n",
    "train_images = images[VALIDATION_SIZE:]\n",
    "train_labels = labels[VALIDATION_SIZE:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Training data therefore is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the CNN Model using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset everything to rerun in jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights and biases initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_init(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=1e-4)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bias_init(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input and Output of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# images\n",
    "x = tf.placeholder('float', shape=[None, image_pixels])\n",
    "# labels\n",
    "y_ = tf.placeholder('float', shape=[None, labels_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 48, 48, 1)\n",
      "(?, 48, 48, 64)\n",
      "(?, 24, 24, 64)\n"
     ]
    }
   ],
   "source": [
    "# first convolutional layer 64\n",
    "W_conv1 = weight_init([5, 5, 1, 64])\n",
    "b_conv1 = bias_init([64])\n",
    "\n",
    "# (27000, 2304) => (27000,48,48,1)\n",
    "image = tf.reshape(x, [-1,image_width , image_height,1])\n",
    "print (image.get_shape()) # =>(27000,48,48,1)\n",
    "\n",
    "\n",
    "z_conv1 = tf.nn.relu(tf.nn.conv2d(image, W_conv1,  strides=[1, 1, 1, 1], padding=\"SAME\") + b_conv1)\n",
    "print (z_conv1.get_shape()) # => (27000,48,48,64)\n",
    "z_pool1 = tf.nn.max_pool(z_conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "print (z_pool1.get_shape()) # => (27000,24,24,1)\n",
    "#h_norm1 = tf.nn.lrn(h_pool1, 4, bias=1.0, alpha=0.001/9.0, beta=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 24, 24, 128)\n"
     ]
    }
   ],
   "source": [
    "# second convolutional layer\n",
    "W_conv2 = weight_init([5, 5, 64, 128])\n",
    "b_conv2 = bias_init([128])\n",
    "\n",
    "z_conv2 = tf.nn.relu(tf.nn.conv2d(z_pool1, W_conv2,  strides=[1, 1, 1, 1], padding=\"SAME\") + b_conv2)\n",
    "print (z_conv2.get_shape()) # => (27000,24,24,128)\n",
    "\n",
    "z_pool2 = tf.nn.max_pool(z_conv2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 12, 12, 128)\n",
      "(?, 6, 6, 128)\n"
     ]
    }
   ],
   "source": [
    "# third convolutional layer\n",
    "W_conv3 = weight_init([5, 5, 128, 128])\n",
    "b_conv3 = bias_init([128])\n",
    "\n",
    "z_conv3 = tf.nn.relu(tf.nn.conv2d(z_pool2, W_conv3,  strides=[1, 1, 1, 1], padding=\"SAME\") + b_conv3)\n",
    "print (z_conv3.get_shape()) # => (27000,12,12,128)\n",
    "\n",
    "z_pool3 = tf.nn.max_pool(z_conv3, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "print (z_pool3.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten the output pool layer to give it to fully connected layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z_pool3_flat = tf.reshape(z_pool3, [-1, 6 * 6 * 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# local layer weight initialization\n",
    "def local_weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.04)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def local_bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 3072)\n"
     ]
    }
   ],
   "source": [
    "# densely connected layer local 3\n",
    "W_fc1 = local_weight_variable([6 * 6 * 128, 3072])\n",
    "b_fc1 = local_bias_variable([3072])\n",
    "# 3072 is a random choice\n",
    "\n",
    "# (27000, 6, 6, 128) => (27000, 6 * 6 * 128)\n",
    "z_pool3_flat = tf.reshape(z_pool3, [-1, 6 * 6 * 128])\n",
    "\n",
    "fc1 = tf.nn.relu(tf.matmul(z_pool3_flat, W_fc1) + b_fc1)\n",
    "print (fc1.get_shape()) # => (27000, 3072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# densely connected layer local 4\n",
    "W_fc2 = local_weight_variable([3072, 1536])\n",
    "b_fc2 = local_bias_variable([1536])\n",
    "\n",
    "# 1536 is a random choice\n",
    "# (40000, 4, 4, 64) => (40000, 3136)\n",
    "z_fc2_flat = tf.reshape(fc1, [-1, 3072])\n",
    "\n",
    "fc2 = tf.nn.relu(tf.matmul(z_fc2_flat, W_fc2) + b_fc2)\n",
    "#print (h_fc1.get_shape()) # => (40000, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dropout\n",
    "keep_prob = tf.placeholder('float')\n",
    "fc2_drop = tf.nn.dropout(fc2, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# readout layer for deep net\n",
    "W_fc3 = weight_init([1536, labels_count])\n",
    "b_fc3 = bias_init([labels_count])\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(fc2_drop, W_fc3) + b_fc3)\n",
    "\n",
    "#print (y.get_shape()) # => (40000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning rate\n",
    "LEARNING_RATE = 1e-4\n",
    "# cost function\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "\n",
    "\n",
    "# optimisation function\n",
    "train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
    "\n",
    "# evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prediction function\n",
    "#[0.1, 0.9, 0.2, 0.1, 0.1 0.3, 0.5, 0.1, 0.2, 0.3] => 1\n",
    "predict = tf.argmax(y,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set to 3000 iterations \n",
    "TRAINING_ITERATIONS = 300\n",
    "    \n",
    "DROPOUT = 0.5\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs_completed = 0\n",
    "index_in_epoch = 0\n",
    "num_examples = train_images.shape[0]\n",
    "\n",
    "# serve data by batches\n",
    "def next_batch(batch_size):\n",
    "    \n",
    "    global train_images\n",
    "    global train_labels\n",
    "    global index_in_epoch\n",
    "    global epochs_completed\n",
    "    \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    # when all training data have been already used, it's reordered randomly  \n",
    "    if index_in_epoch > num_examples:\n",
    "        # finished epoch\n",
    "        epochs_completed += 1\n",
    "        # shuffle the data\n",
    "        perm = np.arange(num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        train_images = train_images[perm]\n",
    "        train_labels = train_labels[perm]\n",
    "        # start next epoch\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "        assert batch_size <= num_examples\n",
    "    end = index_in_epoch\n",
    "    return train_images[start:end], train_labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start TensorFlow session\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualisation variables\n",
    "train_accuracies = []\n",
    "validation_accuracies = []\n",
    "x_range = []\n",
    "\n",
    "display_step=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_accuracy / validation_accuracy => 0.19 / 0.12 for step 0\n",
      "training_accuracy / validation_accuracy => 0.34 / 0.22 for step 1\n",
      "training_accuracy / validation_accuracy => 0.28 / 0.22 for step 2\n",
      "training_accuracy / validation_accuracy => 0.28 / 0.22 for step 3\n",
      "training_accuracy / validation_accuracy => 0.22 / 0.22 for step 4\n",
      "training_accuracy / validation_accuracy => 0.23 / 0.22 for step 5\n",
      "training_accuracy / validation_accuracy => 0.22 / 0.22 for step 6\n",
      "training_accuracy / validation_accuracy => 0.28 / 0.22 for step 7\n",
      "training_accuracy / validation_accuracy => 0.16 / 0.22 for step 8\n",
      "training_accuracy / validation_accuracy => 0.33 / 0.22 for step 9\n",
      "training_accuracy / validation_accuracy => 0.17 / 0.22 for step 10\n",
      "training_accuracy / validation_accuracy => 0.23 / 0.22 for step 20\n",
      "training_accuracy / validation_accuracy => 0.38 / 0.22 for step 30\n",
      "training_accuracy / validation_accuracy => 0.20 / 0.22 for step 40\n",
      "training_accuracy / validation_accuracy => 0.34 / 0.22 for step 50\n",
      "training_accuracy / validation_accuracy => 0.19 / 0.22 for step 60\n",
      "training_accuracy / validation_accuracy => 0.27 / 0.22 for step 70\n",
      "training_accuracy / validation_accuracy => 0.27 / 0.22 for step 80\n",
      "training_accuracy / validation_accuracy => 0.20 / 0.22 for step 90\n",
      "training_accuracy / validation_accuracy => 0.22 / 0.22 for step 100\n",
      "training_accuracy / validation_accuracy => 0.30 / 0.22 for step 200\n",
      "training_accuracy / validation_accuracy => 0.23 / 0.22 for step 299\n"
     ]
    }
   ],
   "source": [
    "for i in range(TRAINING_ITERATIONS):\n",
    "\n",
    "    #get new batch\n",
    "    batch_xs, batch_ys = next_batch(BATCH_SIZE)        \n",
    "\n",
    "    # check progress on every 1st,2nd,...,10th,20th,...,100th... step\n",
    "    if i%display_step == 0 or (i+1) == TRAINING_ITERATIONS:\n",
    "        \n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_xs, \n",
    "                                                  y_: batch_ys, \n",
    "                                                  keep_prob: 1.0})       \n",
    "        if(VALIDATION_SIZE):\n",
    "            validation_accuracy = accuracy.eval(feed_dict={ x: validation_images[0:BATCH_SIZE], \n",
    "                                                            y_: validation_labels[0:BATCH_SIZE], \n",
    "                                                            keep_prob: 1.0})                                  \n",
    "            print('training_accuracy / validation_accuracy => %.2f / %.2f for step %d'%(train_accuracy, validation_accuracy, i))\n",
    "            \n",
    "            validation_accuracies.append(validation_accuracy)\n",
    "            \n",
    "        else:\n",
    "             print('training_accuracy => %.4f for step %d'%(train_accuracy, i))\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        x_range.append(i)\n",
    "        \n",
    "        # increase display_step\n",
    "        if i%(display_step*10) == 0 and i and display_step<100:\n",
    "            display_step *= 10\n",
    "    # train on batch\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_prob: DROPOUT})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check final accuracy on validation set  \n",
    "if(VALIDATION_SIZE):\n",
    "    validation_accuracy = accuracy.eval(feed_dict={x: validation_images, \n",
    "                                                   y_: validation_labels, \n",
    "                                                   keep_prob: 1.0})\n",
    "    print('validation_accuracy => %.4f'%validation_accuracy)\n",
    "    plt.plot(x_range, train_accuracies,'-b', label='Training')\n",
    "    plt.plot(x_range, validation_accuracies,'-g', label='Validation')\n",
    "    plt.legend(loc='lower right', frameon=False)\n",
    "    plt.ylim(ymax = 1.0, ymin = 0.0)\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('step')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(tf.all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver.save(sess, 'my-model1', global_step=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read test data from CSV file \n",
    "test_data = data[data.Usage == \"PublicTest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pixels_values = test_data.pixels.str.split(\" \").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pixels_values = pd.DataFrame(test_pixels_values, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_images = test_pixels_values.values\n",
    "test_images = test_images.astype(np.float)\n",
    "test_images = test_images - test_images.mean(axis=1).reshape(-1,1)\n",
    "test_images = np.multiply(test_images,100.0/255.0)\n",
    "test_images = np.divide(np.subtract(test_images,each_pixel_mean), each_pixel_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('test_images({0[0]},{0[1]})'.format(test_images.shape))\n",
    "\n",
    "# predict test set\n",
    "#predicted_lables = predict.eval(feed_dict={x: test_images, keep_prob: 1.0})\n",
    "\n",
    "# using batches is more resource efficient\n",
    "predicted_lables = np.zeros(test_images.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,test_images.shape[0]//BATCH_SIZE):\n",
    "    predicted_lables[i*BATCH_SIZE : (i+1)*BATCH_SIZE] = \n",
    "    predict.eval(feed_dict={x: test_images[i*BATCH_SIZE : (i+1)*BATCH_SIZE],                                                                                 keep_prob: 1.0})\n",
    "\n",
    "print('predicted_lables({0})'.format(len(predicted_lables)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data.emotion.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(test_data.emotion.values, predicted_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix(test_data.emotion.values, predicted_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(test_data.emotion.values, predicted_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, int(cm[i, j]*100)/100.0,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=False,\n",
    "                      title='Confusion Matrix for Test Dataset')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
